---
layout: post
title:  "Deep Learning Notes I"
author: "tetrisd"
---
This post gives brief insights about feedforward neural networks, specifically covering the topics of intuition and construction. It's mostly drawn from the lucid <i>Deep Learning</i> by Ian Goodfellow, which, despite its name, contains an excellent introduction to classical neural networks.

<h2>Intuition</h2>
Consider multiple linear regression, where we construct a linear model $$y = \beta_0+\beta_1\textbf{x}_1+\beta_2\textbf{x}_2+\cdots+\beta_{n}\textbf{x}_n$$

The multivariate case being

$$\begin{align}
\textbf{y}_i &= \beta_{i0}+\beta_{i1}\textbf{x}_1+\beta_{i2}\textbf{x}_2+\cdots+\beta_{in}\textbf{x}_n\\
\textbf{y} &= \beta\textbf{x}\ + \beta_0
\end{align}$$

It's clear that this model is a specific case of a single-layer feedforward neural network with the identity activation function. Recall that an $n$-layer feedforward neural network has the form

$$f(\textbf{x};\pmb{\theta}) = h_n \circ h_{n-1} \circ \cdots \circ h_{1}(\textbf{x})$$
$$h_i(\textbf{z}) = g_i(W^{(i)}\textbf{z}+\textbf{b}_i)$$

If we take $n=1$, $W^{(1)}=\beta$, $g_1(\textbf{z})=\textbf{z}$, and $\textbf{b}_1=\beta_0$, the neural network is equivalent to multivariate linear regression. Thus, linear regression is a special case of feedforward neural networks.
<h3>Learning parameters</h3>
We essentially wish to optimize a (difficult) objective function associated with the neural network to find the parameter set $\pmb{\theta}$. Extending the linear regression case again, one such function appears to be MSE, especially in the least-squared and maximum likelihood sense. However, this extends poorly to neural networks: in practice, squaring magnifies numerical errors, leading to <b>saturation</b>, where the gradient becomes nearly zero due to large values in certain activation functions. This effectively halts the optimization process.
<br><br>
The solution is to use a negative log-likelihood cost function (cross-entropy). Since many activation functions (sigmoid, softmax, etc) contain exponentiation, we can "undo" the exp by using a negative log-likelihood, thereby reducing numerical error and saturation.

<h3>Parameter initialization</h3>
How should we <a href="http://cs231n.github.io/neural-networks-2/#weight-initialization">seed $\pmb\theta$</a> for training? Clearly, the default value of 0 results in the same gradient and parameter updates everywhere. A common practice is to initialize all weights to small random numbers with mean 0. However, done so haphazardly, this also increases the variance of the outputs w.r.t. the number of inputs of that layer:

$$\begin{align} \text{Var}\left(\sum_i^n w x_i + b\right) &= \sum_i^n \text{Var}(w x_i)\\
&= \sum_i^n \text{Var}(w)\text{Var}(x_i) + E\left[x_i\right]^2 \text{Var}(w) + E\left[w\right]^2 \text{Var}(x_i)\\
&= \sum_i^n \text{Var}(w)\text{Var}(x_i)\\
&= n \text{Var}(w) \sum_i^n \text{Var}(x_i)
\end{align}
$$

To satisfy $n\text{Var}(w) = k$ for some constant $k$, $\text{Var}(w)$ must equal $1/n$. Thus, we set $w := w/\sqrt n$.
<br><br>
There are also the bias parameters that we should consider. 0 is sane, but in the case of ReLU there are mixed suggestions. <i>Deep Learning</i> suggests a small positive bias to force ReLU activation, while <a href="http://cs231n.github.io/neural-networks-2/#weight-initialization">CS231n</a> questions it. As mentioned in CS231n, <a href="https://arxiv.org/pdf/1502.01852.pdf">state-of-the-art</a> recommends $w\sim U[0, 1]$ then $w := w \sqrt{2/n}$ for PReLU, a generalization of "leaky" RELU.
