---
author: tetrisd
title: Exploiting Structure in Solving Linear Systems
layout: post
---
Given an arbitrary matrix $A_{n \times n}$ for <em>directly </em>solving $Ax=b$, we can hardly do better than standard Gaussian elimination (GE), which runs in $\frac{2}{3}n^3+O(n^2)$ FLOPs. But what if we knew that $A$ was sparse, banded, or symmetric positive-definite? We can often use the structure of the matrix to our advantage and achieve memory and time savings, especially since efficient structures arise in many practical applications.
<br>
<h2>Symmetric Matrices</h2>
A square matrix $A_{n \times n}$ is said to be symmetric if $a_{ij} = a_{ji}$. This property alone does not imply time savings: we further require that all leading principal submatrices of $A$ are non-singular. In this case, $A$ can be factorized as $LDL^T$--where $L$ is a lower triangular matrix and $D$ is a diagonal matrix--and the derivation would involve $A = LDM^T$ and $A = LU$. It has FLOPs $= \frac{1}{3}n^3+O(n^2)$, a linear saving of $\frac{1}{2}$ over LU decomposition, since we are effectively only computing the lower triangular matrix in $LU$.
<br><br>
If we also require $A$ to be positive-definite ($x^TAx &gt; 0, \forall x \ne \textbf{0}$), then by the Cholesky decomposition $A = LL^T$. This has the same time complexity as computing $A = LDL^T$.
<h2>Band Matrices</h2>
A square matrix $A_{n \times n}$ is banded if $a_{ij} = 0$ for an upper bandwidth $p$ and a lower bandwidth $q$ such that $a_{ij} = 0$ for all  $j &gt; i + p$ and $i &gt; j + q$. It is clear that $Ax = b$ can be solved in $O(npq)$; entries past the $p$ and $q$ "borders" in each iteration of GE can be ignored.
<h2>Sparse Symmetric Matrices</h2>
Consider a symmetric matrix $A_{n \times n}$. It is also sparse if there are more zeros than non-zero entries in $A$. We can try to be more memory efficient by using different orderings of variables to reduce fill-in, which is the number of non-zero entries introduced in GE. That is, we permute the columns and rows of $Ax = b$ without changing the problem. Finding an optimal ordering of variables is NP-complete, so we resort to heuristics in most practical applications.
<br><br>
It is helpful to view $A$ as a graph for visualizing these algorithms, where there is a corresponding graph $G = (V, E)$ of $A$ such that $|G|=n$ and $(i, j) \in E \iff a_{ij} \neq 0$. We then introduce several theorems to shine light on how $G$ and $A$ are related.
<br><br>
<em>Theorem 1: </em>Renumbering $V$ corresponds to reordering $A$. In other words, numbering $G$ differently while maintaining the same structure corresponds to reordering the columns and rows of $A$.
<br><br>
<em>Theorem 2: </em>At the end of the $i$th iteration of GE on $A$, $v_i$ is removed from $G$ and $E = E \cup L$ such that $L$ forms the complete graph of $N(v_i)$. That is, $v_i$ is removed and all its neighbors form a complete graph. The additional edges denote the <em>fill-in</em> for the $i$th iteration of GE.
<br><br>
Given these theorems, we have insight into how the following fill-in reduction algorithms work:
<br><br>
<strong>Minimum vertex degree ordering: </strong>Choose the vertices so that $deg(i) \leq $deg(j), j &gt; i$. By <i>Theorem 1,</i> the number of non-zero (NNZ) entries of row $i &lt; $ NNZ entries of row $j$. Intuitively, we now choose a locally optimal node for each iteration of GE.
<br><br>
<strong>Cuthill-McKee: </strong>Run breadth-first search on the minimum vertex and, for each level, order the discovered nodes by their degree.

